\section{Study Setup}
\label{study_setup}
% \section{Study Setup}
% overview
% \subsection{Participants}
% how the participants are selected, what is the response rate? 
% any incentives were provided to encourage participation?
% \subsection{Data Collection and Analysis}
% how was the data collected (Google form), how was the data analyzed (open coding for open-ended questions, statistical analysis for closed questions) see \url{https://ieeexplore.ieee.org/abstract/document/8658125}

For this study, we have conducted an interview session and a survey. The goal of the interview session was to reduce ambiguity from the survey questions. In total, eight individual participants from four leading software companies were randomly selected for the interview sessions. In these sessions, the participants first completed the survey, and then we interviewed participants to identify ambiguities in the question. From the feedback and results of the interview session, we revised the questions. The original survey was conducted through Google Forms.

\subsection{Participants}
\label{survey_participants}
In this survey, we have targeted developers who are currently working in the software industry of Bangladesh. We applied purposive sampling\cite{Vogt2005} to include respondents in a software development related role. Purposive sampling is basically based on the assumption of the population. It is possible that some elements will not have a chance of selection in this method. Moreover, the probability of selection can't be accurately determined in this process. \anindya{May be elaborated with 1/2 more lines.}\partha{added} We shared the survey link through the authors' personal connection and in the local developers' groups on social media to achieve our sampling goal. We also implemented the chain referral strategy\cite{creswell2013} and asked others to pass on the survey invite. Following this strategy, we reached out to an unknown number of potential respondents. Hence, it was not possible to calculate the response rate of our survey. Moreover, some elements of the population (who do not use social media) may be excluded from the survey. In total, we have received 137 responses from the survey. 

The current role of the participants are: 
\input{Tables/participants_role}

The distribution of experience of the participants are:
\input{Tables/participants_experience}

Therefore, at least 61.31\% respondents in our survey have worked in the industry for at least 2 years.

\subsection{Interviews}
\label{interviews}
For the interviews, we have conducted an interview session of about half an hour for each interviewee. Interviewees were first asked to complete the survey. After completing the survey, we asked what he/she understood from the questions and what he/she meant by the answers. From the interviews, we identified discrepancies between the understanding of the interviewee and the goals of the survey questions. The survey questions were then adjusted to reduce ambiguity among them.
\partha{It seems the interviews were not structured, and the purpose of the interviews is to find ambiguity. Thus, it is hard to create a detailed analysis of the interviews like the SRBD review paper.}

\subsection{Data Collection \& Analysis}
\label{survey_data_collection}
We have conducted the survey through Google Forms. The survey link was opened before the invitations were sent, and the survey link was closed for two consecutive weeks without any response. The survey link was open for feedback for about two months. 
\input{Tables/Agreemnet_Level}
A systematic qualitative data analysis process was followed to analyze the open-ended questions. First, the two authors independently coded the first 30\% responses of each question's to extract potential categories. Second, the authors conducted discussion sessions to develop a unified common coding scheme for each question using these categories. Third, the rest of the responses were coded using this coding scheme using the Coding Analysis Toolkit (CAT)\cite{Lu2008} software. To measure the level of agreement between two coders, we used the online tool Recal2\cite{Recal2020} and CAT\cite{Lu2008}. The Recal2 calculator reports the agreement using four measures: 1) Percent agreement, 2) Cohen $\kappa$\cite{Cohen1960}, 3) Scott’s $\pi$\cite{scott1955}, and 4) Krippendorff’s $\alpha$\cite{krippendorff2004}. It is believed that Krippendorff’s $\alpha$ is more sensitive to bias introduced by a coder and is recommended\cite{Joyce2013} over Cohen's $\kappa$\cite{Cohen1960}. The level of agreement is presented in Table \ref{table: agreement level}. The average $\kappa$ value was 0.71. It is a common practice that Cohen's $\kappa$ value between 0.61 and 0.80\cite{Landis1977} is considered a `substantial agreement.’ In the coding process, a large number of codes were generated from each of the open-ended questions. To help with our analysis, we conducted discussion sessions to identify the codes that express similar themes. After reaching a consensus, we grouped those codes into a smaller number of high-level categories. We have used the statsmodels\cite{seabold2010} and scipy\cite{scipy2020} modules in Python for statistical analysis. In the survey question, we had an optional question regarding the name of the company. Most of the participants answered the questions. Using the authors' personal connections and LinkedIn data, we have collected company size information from the company name. Later, we categorized the companies using the criteria presented in Table \ref{table: size criteria}.
\partha{For open-ended questions, we have followed the methodology of the \href{https://link.springer.com/article/10.1007/s10664-019-09708-7}{blockchain paper}. The methodology of the opiner paper (`card') seems a bit different.}\partha{Other metrics of measuring agreement level is added}
\input{Tables/Size_Criteria}