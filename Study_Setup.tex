\section{Study Setup}
\label{study_setup}
% \section{Study Setup}
% overview
% \subsection{Participants}
% how the participants are selected, what is the response rate? 
% any incentives were provided to encourage participation?
% \subsection{Data Collection and Analysis}
% how was the data collected (Google form), how was the data analyzed (open coding for open-ended questions, statistical analysis for closed questions) see \url{https://ieeexplore.ieee.org/abstract/document/8658125}

Our study has four steps as follows:
\begin{itemize}[leftmargin=10pt]
  \item\bf{Step 1 - Interview.}  We conduct a series of semi-structured interviews to get an overview of the current development practices in Bangladesh and to understand how 
  such practices could differ from other countries.
  \item\bf{Step 2 - Survey.} We design a survey questions to get deep insights on the insights collected from the interviews. 
  \item\bf{Steps 3, 4 - Data Analysis to answer RQ1 and RQ2.} We analyze the survey responses to answer RQ1 (i.e., understand development practices in Bangladesh) and RQ2 (compare the practices in Bangladesh with other countries).
\end{itemize} We discuss the steps below.
\subsection{Interview}
The goal of
the interview session was to prepare the survey questions. In
total, eight individual participants from four leading software companies were
selected. First we designed an initial list of survey questions in Google form by consulting previous studies in other countries like 
Canada, Turkey, Netherlands, New Zealand, etc.~\cite{Garousi2013, Garousi2015, Vonken2012, Wang2018}. Each
participant was asked
identify ambiguities in the question. From the feedback and results of the
interview session, we revised the questions. Each interview session lasted about half an hour. 
Interviewees were first asked to complete the survey.
After completing the survey, we asked what he/she understood from the questions
and what he/she meant by the answers. Throughout the interviews, we identified
discrepancies between the understanding of the interviewee and the goals of the
survey questions by comparing the interview findings against findings from related work (i.e., findings from other countries). 
%Finally, we adjusted the survey questions to reduce ambiguity in survey questions.


\subsection{Survey}
\label{survey_participants}
In this survey, we have targeted developers who are currently working in the
software industry of Bangladesh. We applied purposive sampling~\cite{Vogt2005} to
include respondents in a software development related role. Purposive sampling
is basically based on the assumption of the population. It is possible that some
elements will not have a chance of selection in this method. Moreover, the
probability of selection can't be accurately determined in this process. We
shared the survey link through the authors' personal connection and in the local
developers' groups on social media to achieve our sampling goal. We also
implemented the chain referral strategy~\cite{creswell2013} and asked others to
pass on the survey invite. Due to such snowball approach to recruit survey participants, it is not possible to calculate the
response rate of our survey. We have conducted the survey through Google Forms. The survey link was opened
before the invitations were sent, and the survey link was closed for two
consecutive weeks without any response. The survey link was open for feedback
for about two months. In total, we
received 137 responses from the survey. Each participant was first asked a series of demographic questions (e.g., roles, experience, gender) and 
then was presented the survey questions related to the development practices. \tbl\ref{tab:role} shows the distribution of the survey participants by their roles. More than 69\% of the participants 
are software developers and 16.9\% are managers. Around 8\% repoted as being other kinds of software engineers (e.g., data engineer, R\&D engineer). 
\tbl\ref{tab:experience} shows the distribution of the survey participants by their experience. More than 61\% of the participants worked in the
industry for at least 2 years.
 
\input{Tables/participants_role}
\input{Tables/participants_experience}

% 
% \subsection{Interviews}
% \label{interviews}
% For the interviews, we have conducted an interview session of about half an hour
% for each interviewee. Interviewees were first asked to complete the survey.
% After completing the survey, we asked what he/she understood from the questions
% and what he/she meant by the answers. From the interviews, we identified
% discrepancies between the understanding of the interviewee and the goals of the
% survey questions. The survey questions were then adjusted to reduce ambiguity
% among them.

\subsection{Data Analysis to Answer RQ1 and RQ2}
\label{survey_data_collection}
The survey consisted of both closed and open-ended questions. 
We analyzed the closed questions using standard descriptive and statistical techniques. We analyzed the 
closed questions following principles of open coding. Open coding includes labelling of concepts/
categories in textual contents based on the properties and
dimensions of the development entities (e.g., tools, processes, etc.) about which the contents
are provided. A systematic qualitative data analysis process was followed to analyze the
open-ended questions. First, the two authors independently coded the first 30\%
responses of each question's to extract potential categories. Second, the
authors conducted discussion sessions to develop a unified common coding scheme
for each question using these categories. Third, the rest of the responses were
coded using this coding scheme using the Coding Analysis Toolkit
(CAT)\cite{Lu2008} software. To measure the level of agreement between two
coders, we used the online tool Recal2\cite{Recal2020} and CAT\cite{Lu2008}. The
Recal2 calculator reports the agreement using four measures: 1) Percent
agreement, 2) Cohen $\kappa$\cite{Cohen1960}, 3) Scott’s $\pi$\cite{scott1955},
and 4) Krippendorff’s $\alpha$\cite{krippendorff2004}. It is believed that
Krippendorff’s $\alpha$ is more sensitive to bias introduced by a coder and is
recommended\cite{Joyce2013} over Cohen's $\kappa$\cite{Cohen1960}. The level of
agreement is presented in Table \ref{table: agreement level}. The average
$\kappa$ value was 0.71. It is a common practice that Cohen's $\kappa$ value
between 0.61 and 0.80\cite{Landis1977} is considered a `substantial agreement.’
In the coding process, a large number of codes were generated from each of the
open-ended questions. To help with our analysis, we conducted discussion
sessions to identify the codes that express similar themes. After reaching a
consensus, we grouped those codes into a smaller number of high-level
categories. We have used the statsmodels\cite{seabold2010} and
scipy\cite{scipy2020} modules in Python for statistical analysis. \gias{can we say something about the distribution of the 137 participants by 
the total number of software companies and the types of the software companies?}
% In the survey
% question, we had an optional question regarding the name of the company. Most of
% the participants answered the questions. Using the authors' personal connections
% and LinkedIn data, we have collected company size information from the company
% name. Later, we categorized the companies using the criteria presented in Table
% \ref{table: size criteria}.
% \partha{For open-ended questions, we have followed the methodology of the
% \href{https://link.springer.com/article/10.1007/s10664-019-09708-7}{blockchain
% paper}. The methodology of the opiner paper (`card') seems a bit
% different.}\partha{Other metrics of measuring agreement level is added}
\input{Tables/Agreemnet_Level}
%\input{Tables/Size_Criteria}
